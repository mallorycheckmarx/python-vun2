To allow multiplexing of the HTTP servers to take place, a load balancer (specifically, HaProxy) is placed before HTTP servers.
HaProxy also allows us to distinguish between traffic intended for different services, which is useful for serving media and
static content on a standalone development machine. This can be done since HaProxy is a layer 4 load balancer, meaning it can look into the HTTP headers and decide where a request goes.
Memcached is multiplexed by using mcrouter (https://github.com/facebook/mcrouter), which acts as a gate for memcached access. This allows to have multiple memcached instances acting as shards, at the expense of another layer of indirection and some possibility of "chaos" whenever something causes the hashing scheme to change (such as memcached instance going down).
For deployment, a replicate of the main Postgres databases is used for queued job processing. The reasoning is that these databases are often the bottleneck in response time for regular HTTP requests. Much work in the architecture is devoted to mitigate the number of queries these databases receive.
However, in the asynchorous part of the architecture, for queue jobs, a lot of heavy database queries are needed. Because the queue jobs have no response time requirements, the machines involved in the queue processing can be run as quickly as possible, and therefore can hit the databases as mercilessly as they can.
As this is undesirable for the main databases, a replica set is used that is mirrored periodically. Of course, a trade-off here is that the queue jobs might occasionally work on outdated data. This is mostly mitigated by memcached, which has a cache timeout larger than the replication period, and thus is likely to have recent data for which the replica databases are outdated.
Cassandra can scale to any number of machines if need-be, due to the ring cluster architecture it uses. Cassandra instances are assigned a token that determines their slot in the ring. The key used on data fetches maps to a region in the ring. Since Cassandra only stores computed data, there's no concern for data replication, which simplifies scaling up/down the number of machines.